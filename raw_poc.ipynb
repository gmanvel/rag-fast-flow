{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60458ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH=\"/Users/gmanvel/repos/rag-fast-flow/data/fast_flow.pdf\"\n",
    "JSON_OUTPUT_PATH=\"/Users/gmanvel/repos/rag-fast-flow/data/fast_flow_extracted.json\"\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f33061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "HEADER_SIZE = 34\n",
    "SECTION_SIZE = 18\n",
    "CONTENT_SIZE = 13\n",
    "TOL = 0\n",
    "\n",
    "def dominant_size(block):\n",
    "    sizes = []\n",
    "    for line in block.get(\"lines\", []):\n",
    "        for span in line.get(\"spans\", []):\n",
    "            sizes.append(span.get(\"size\"))\n",
    "    if not sizes:\n",
    "        return None\n",
    "    rounded = [round(s, 1) for s in sizes if s is not None]\n",
    "    if not rounded:\n",
    "        return None\n",
    "    freq = {}\n",
    "    for s in rounded:\n",
    "        freq[s] = freq.get(s, 0) + 1\n",
    "    return max(freq.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "def block_text(block):\n",
    "    parts = []\n",
    "    for line in block.get(\"lines\", []):\n",
    "        for span in line.get(\"spans\", []):\n",
    "            parts.append(span.get(\"text\", \"\"))\n",
    "    text = \"\".join(parts)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s*\\n\\s*\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def classify(size):\n",
    "    if size is None:\n",
    "        return None\n",
    "    if abs(size - HEADER_SIZE) <= TOL:\n",
    "        return \"header\"\n",
    "    if abs(size - SECTION_SIZE) <= TOL:\n",
    "        return \"section\"\n",
    "    if abs(size - CONTENT_SIZE) <= TOL:\n",
    "        return \"content\"\n",
    "    return None\n",
    "\n",
    "def sanitize_str(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "\n",
    "def sanitize(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {sanitize(k): sanitize(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [sanitize(x) for x in obj]\n",
    "    if isinstance(obj, str):\n",
    "        return sanitize_str(obj)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38d9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(FILE_PATH)\n",
    "\n",
    "point = []\n",
    "current_header = None\n",
    "current_section = None\n",
    "\n",
    "for page in doc[2:64]:\n",
    "    data = page.get_text(\"dict\")\n",
    "    blocks = data.get(\"blocks\", [])\n",
    "    blocks_sorted = sorted(blocks, key=lambda b: (b.get(\"bbox\", [0,0,0,0])[1], b.get(\"bbox\", [0,0,0,0])[0]))\n",
    "    for b in blocks_sorted:\n",
    "        if b.get(\"type\") != 0:\n",
    "            continue\n",
    "        size = dominant_size(b)\n",
    "        kind = classify(size)\n",
    "        if kind is None:\n",
    "            continue\n",
    "        text = block_text(b)\n",
    "        if not text:\n",
    "            continue\n",
    "        if kind == \"header\":\n",
    "            current_header = {\"header\": text, \"sections\": []}\n",
    "            point.append(current_header)\n",
    "            current_section = None\n",
    "        elif kind == \"section\":\n",
    "            if current_header is None:\n",
    "                current_header = {\"header\": \"\", \"sections\": []}\n",
    "                point.append(current_header)\n",
    "            current_section = {\"tile\": text, \"content\": \"\"}\n",
    "            current_header[\"sections\"].append(current_section)\n",
    "        elif kind == \"content\":\n",
    "            if current_section is None:\n",
    "                if current_header is None:\n",
    "                    current_header = {\"header\": \"\", \"sections\": []}\n",
    "                    point.append(current_header)\n",
    "                current_section = {\"tile\": \"\", \"content\": \"\"}\n",
    "                current_header[\"sections\"].append(current_section)\n",
    "            if current_section[\"content\"]:\n",
    "                current_section[\"content\"] += text #\"\\n\" + text\n",
    "            else:\n",
    "                current_section[\"content\"] = text\n",
    "\n",
    "for h in point:\n",
    "    h[\"header\"] = h[\"header\"].strip()\n",
    "    cleaned_sections = []\n",
    "    for s in h[\"sections\"]:\n",
    "        s[\"tile\"] = s.get(\"tile\", \"\").strip()\n",
    "        s[\"content\"] = s.get(\"content\", \"\").strip()\n",
    "        if s[\"tile\"] or s[\"content\"]:\n",
    "            cleaned_sections.append(s)\n",
    "    h[\"sections\"] = cleaned_sections\n",
    "\n",
    "point = sanitize(point)\n",
    "\n",
    "#result\n",
    "\n",
    "# import json\n",
    "# with open(JSON_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(result, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45283474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29219f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4-turbo\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "for header in point:    \n",
    "    for section in header[\"sections\"]:\n",
    "        section[\"token_count\"] = count_tokens(section.get(\"content\", \"\"))\n",
    "\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain_experimental\n",
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import json\n",
    "\n",
    "with open(JSON_OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all content from the sections\n",
    "texts = []\n",
    "for header in data:\n",
    "    for section in header[\"sections\"]:\n",
    "        if section.get(\"content\"):\n",
    "            texts.append(section[\"content\"])\n",
    "\n",
    "print(f\"Extracted {len(texts)} text sections.\")\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "documents = text_splitter.create_documents(texts=texts, metadatas=[{} for _ in texts])\n",
    "print(f\"Created {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73dd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Initialize OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Load the JSON data\n",
    "with open(JSON_OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process sections and create embeddings\n",
    "sections_with_embeddings = []\n",
    "\n",
    "for header in data:\n",
    "    for section in header[\"sections\"]:\n",
    "        if section.get(\"tile\"):  # Only process sections with titles\n",
    "            # Create embedding for the content\n",
    "            embedding_vector = embeddings.embed_query(section[\"content\"])\n",
    "            \n",
    "            # Create a section object with title, text, and embedding\n",
    "            section_obj = {\n",
    "                \"title\": section[\"tile\"],\n",
    "                \"text\": section[\"content\"],\n",
    "                \"embedding\": embedding_vector\n",
    "            }\n",
    "            \n",
    "            sections_with_embeddings.append(section_obj)\n",
    "\n",
    "print(f\"Created embeddings for {len(sections_with_embeddings)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345b92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sections_with_embeddings[0][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "output_file = \"/Users/gmanvel/repos/rag-fast-flow/data/sections_with_embeddings.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sections_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved embeddings to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jrs8woicmd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6rj14nn4bzi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Connect to local Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Verify connection\n",
    "print(f\"Connected to Qdrant at localhost:6333\")\n",
    "print(f\"Collections: {client.get_collections()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o4xuusnev9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "collection_name = \"fast_flow_sections\"\n",
    "\n",
    "# Recreate collection (delete if exists)\n",
    "try:\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "    print(f\"Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing collection to delete: {e}\")\n",
    "\n",
    "# Create new collection\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"Created collection '{collection_name}' with vector size 768 and cosine distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcir98nninn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "import json\n",
    "\n",
    "# Load embeddings from JSON\n",
    "embeddings_file = \"/Users/gmanvel/repos/rag-fast-flow/data/sections_with_embeddings.json\"\n",
    "with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "    sections_data = json.load(f)\n",
    "\n",
    "# Prepare points for insertion\n",
    "points = []\n",
    "for idx, section in enumerate(sections_data):\n",
    "    point = PointStruct(\n",
    "        id=idx,\n",
    "        vector=section[\"embedding\"],\n",
    "        payload={\n",
    "            \"title\": section[\"title\"],\n",
    "            \"text\": section[\"text\"]\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "# Insert points into Qdrant\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(f\"Successfully inserted {len(points)} points into '{collection_name}' collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htyn299fkvp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify insertion by getting collection info\n",
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "print(f\"Collection '{collection_name}' info:\")\n",
    "print(f\"  - Total points: {collection_info.points_count}\")\n",
    "print(f\"  - Vector size: {collection_info.config.params.vectors.size}\")\n",
    "print(f\"  - Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "\n",
    "# Test a simple search with the first embedding\n",
    "if len(sections_data) > 0:\n",
    "    print(\"\\nTesting search with first section's embedding...\")\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=sections_data[0][\"embedding\"],\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop 3 similar sections:\")\n",
    "    for i, point in enumerate(search_results, 1):\n",
    "        print(f\"\\n{i}. Score: {point.score:.4f}\")\n",
    "        print(f\"   Title: {point.payload['title']}\")\n",
    "        print(f\"   Text preview: {point.payload['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d555ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embeddings.embed_query(\"What is the Wardley doctrine?\")\n",
    "waldey_doctrine = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=3,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 3 similar sections:\")\n",
    "for i, point in enumerate(waldey_doctrine.points, 1):\n",
    "    print(f\"\\n{i}. Score: {point.score:.4f}\")\n",
    "    print(f\"   Title: {point.payload['title']}\")\n",
    "    print(f\"   Text preview: {point.payload['text']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeeb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_index\n",
    "%pip install llama_index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9998cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97937bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "collection_name = \"fast_flow\"\n",
    "\n",
    "# Create new collection\n",
    "# client.create_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "# )\n",
    "ollama_embeddings = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=70,\n",
    "    embed_model=ollama_embeddings\n",
    ")\n",
    "\n",
    "points: list[PointStruct] = []\n",
    "for index, section in enumerate(sections_data):\n",
    "    #print(f\"Processing section title: {section['title']}\")\n",
    "    #print(f\"Content:{section['text']}\")\n",
    "    nodes = splitter.get_nodes_from_documents(documents=[Document(text=section[\"text\"])])\n",
    "    chunks = [(node.embedding, node.get_content()) for node in nodes]\n",
    "\n",
    "    #print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    for inner_index, (_, content) in enumerate(chunks):\n",
    "        if not content.strip() or content.strip() == \"Summary\":\n",
    "            continue\n",
    "        emb = ollama_embeddings.get_text_embedding(content)\n",
    "        point = PointStruct(\n",
    "            id=index*10 + inner_index,\n",
    "            vector=emb,\n",
    "            payload={\n",
    "                \"title\": section[\"title\"],\n",
    "                \"text\": content\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "        #print(f\"Embedding (first 5 values): {emb[:5]}\")\n",
    "        #print(f\"Content: {content}\\n\")\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import QueryResponse\n",
    "\n",
    "query_vector = ollama_embeddings.get_text_embedding(\"What is the Wardley doctrine?\")\n",
    "waldey_doctrine: QueryResponse = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=3\n",
    ")\n",
    "print(f\"\\nTop 3 similar sections:\")\n",
    "for i, point in enumerate(sorted(waldey_doctrine.points, key=lambda p: p.score, reverse=True), 1):\n",
    "    print(f\"\\n{i}. Score: {point.score:.4f}\")\n",
    "    print(f\"   Title: {point.payload['title']}\")\n",
    "    print(f\"   Text preview: {point.payload['text']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739efae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#waldey_doctrine.points[1].payload['text']\n",
    "%pip install -U llama-index-core==0.11.15 llama-index-llms-ollama==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata\n",
    "\n",
    "for pkg in [\n",
    "    \"llama-index-core\",\n",
    "    \"llama-index-llms-ollama\",\n",
    "    \"llama-index-embeddings-ollama\",\n",
    "    \"llama-index\",\n",
    "]:\n",
    "    try:\n",
    "        print(pkg, importlib.metadata.version(pkg))\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(pkg, \"NOT INSTALLED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc16b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"mistral\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"You are a consultant specializing in fast flow methodologies using the Wardley mappings, DDD & Team Topologies. You are hired to consult and explain concepts around fast flow concepts.\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"What is the Wardley doctrine?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(f\"Response from LLM: {response.message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10316854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import QueryResponse\n",
    "\n",
    "collection_name = \"fast_flow\"\n",
    "\n",
    "# Connect to local Qdrant instance\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "ollama_embeddings = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "user_query = \"What is the Doctrine in Wardley Maps?\"\n",
    "query_vector = ollama_embeddings.get_text_embedding(user_query)\n",
    "wardley_doctrine: QueryResponse = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content= (\n",
    "            f\"You are a consultant specializing in fast flow methodologies using the Wardley mappings, DDD & Team Topologies.\\n\"\n",
    "            f\"You are hired to consult and explain concepts around fast flow concepts.\"\n",
    "            f\"Context from your notes:{wardley_doctrine.points[0].payload['text']}\"\n",
    "        )\n",
    "\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=user_query\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(f\"Response from LLM: {response.message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
